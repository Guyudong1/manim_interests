{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Visualization with Manim\n",
    "\n",
    "This notebook demonstrates gradient descent optimization for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manim import BOLD\n",
    "from mymanim import *\n",
    "import numpy as np\n",
    "config.frame_width = 25\n",
    "config.frame_height = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Animation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionDemo(Scene):\n",
    "    def construct(self):\n",
    "        # 1. Generate two classes of 2D points (linearly separable)\n",
    "        np.random.seed(0)\n",
    "        n_points = 20\n",
    "        X1 = np.random.randn(n_points, 2) * 1.0 + np.array([2, 2])  # Class 1 points\n",
    "        X2 = np.random.randn(n_points, 2) * 1.0 + np.array([6, 6])  # Class 2 points\n",
    "        X = np.vstack([X1, X2])\n",
    "        y = np.array([0]*n_points + [1]*n_points)\n",
    "\n",
    "        # 2. Create axes\n",
    "        axes = Axes(\n",
    "            x_range=[-2, 10, 1],\n",
    "            y_range=[-2, 10, 1],\n",
    "            axis_config={\"color\": WHITE},\n",
    "            x_length=10,\n",
    "            y_length=10,\n",
    "        ).move_to(LEFT*4)  # Move axes to the left\n",
    "        self.play(Create(axes))\n",
    "\n",
    "        # 3. Plot data points\n",
    "        dots1 = VGroup(*[Dot(axes.c2p(*pt), color=BLUE) for pt in X1])  # Class 0 points\n",
    "        dots2 = VGroup(*[Dot(axes.c2p(*pt), color=RED) for pt in X2])   # Class 1 points\n",
    "        self.play(FadeIn(dots1, lag_ratio=0.1), FadeIn(dots2, lag_ratio=0.1))\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # 4. Initialize decision boundary parameters\n",
    "        w_tracker = ValueTracker(1)    # w1 coefficient\n",
    "        v_tracker = ValueTracker(-1)   # w2 coefficient\n",
    "        b_tracker = ValueTracker(0)    # bias term\n",
    "\n",
    "        # 5. Create decision boundary line\n",
    "        def get_line():\n",
    "            w1 = w_tracker.get_value()\n",
    "            w2 = v_tracker.get_value()\n",
    "            b0 = b_tracker.get_value()\n",
    "            if abs(w2) < 1e-3:\n",
    "                w2 = 1e-3\n",
    "            if abs(w1) < 1e-3:\n",
    "                w1 = 1e-3\n",
    "            def y_func(x): return -(w1/w2)*x - b0/w2\n",
    "            x_vals = []\n",
    "            for x in np.linspace(0, 8, 200):\n",
    "                y = y_func(x)\n",
    "                if 0 < y < 8:\n",
    "                    x_vals.append(x)\n",
    "            # Calculate valid range\n",
    "            if len(x_vals) < 2:\n",
    "                return VGroup()\n",
    "            length = x_vals[-1] - x_vals[0]\n",
    "            # Fade line when too short\n",
    "            if length < 0.5:\n",
    "                alpha = max(0.1, length / 0.5)\n",
    "            else:\n",
    "                alpha = 1.0\n",
    "            return axes.plot(y_func, color=YELLOW, x_range=[x_vals[0], x_vals[-1]], stroke_opacity=alpha)\n",
    "        \n",
    "        sep_line = always_redraw(get_line)\n",
    "        self.add(sep_line)\n",
    "\n",
    "        # Helper functions for logistic regression\n",
    "        def sigmoid(z):\n",
    "            return 1/(1+np.exp(-z))\n",
    "            \n",
    "        def get_loss():\n",
    "            w1 = w_tracker.get_value()\n",
    "            w2 = v_tracker.get_value()\n",
    "            b0 = b_tracker.get_value()\n",
    "            z = w1*X[:,0] + w2*X[:,1] + b0\n",
    "            y_pred = sigmoid(z)\n",
    "            loss = -np.mean(y*np.log(y_pred+1e-8)+(1-y)*np.log(1-y_pred+1e-8))\n",
    "            return loss\n",
    "\n",
    "        # Track loss history\n",
    "        loss_history = ValueTracker(get_loss())\n",
    "\n",
    "        # Counter for unchanged loss\n",
    "        unchanged_count = [0]\n",
    "        last_loss = [get_loss()]\n",
    "\n",
    "        def get_loss_color():\n",
    "            curr_loss = get_loss()\n",
    "            if abs(curr_loss - last_loss[0]) < 1e-8:\n",
    "                unchanged_count[0] += 1\n",
    "            else:\n",
    "                unchanged_count[0] = 0\n",
    "            last_loss[0] = curr_loss\n",
    "            # Turn yellow if loss hasn't changed for 10 steps\n",
    "            if unchanged_count[0] >= 10:\n",
    "                return YELLOW\n",
    "            else:\n",
    "                return WHITE\n",
    "\n",
    "        # Dynamic loss display below axes\n",
    "        loss_text = always_redraw(lambda: Text(\n",
    "            f\"Current Loss: {get_loss():.3f}\", font_size=44, color=get_loss_color(), weight=BOLD\n",
    "        ).next_to(axes, DOWN, buff=0.5))\n",
    "        self.add(loss_text)\n",
    "\n",
    "        # Learning rate display\n",
    "        lr = 0.1\n",
    "        lr_text = always_redraw(lambda: Text(\n",
    "            f\"Learning Rate: {lr}\", font_size=44, color=WHITE, weight=BOLD\n",
    "        ).next_to(axes, UP, buff=0.5))\n",
    "        self.add(lr_text)\n",
    "\n",
    "        # Loss curve visualization\n",
    "        steps = 200\n",
    "        loss_history_list = []\n",
    "        max_loss = get_loss()\n",
    "        loss_axes = Axes(\n",
    "            x_range=[0, steps, 10],\n",
    "            y_range=[0, max_loss*1.5 if max_loss > 0 else 1.5, max_loss/5 if max_loss > 0 else 0.3],\n",
    "            x_length=8,\n",
    "            y_length=4,\n",
    "        )\n",
    "        loss_axes.next_to(axes, RIGHT, buff=1.2)\n",
    "        loss_axes.align_to(axes, DOWN)\n",
    "        loss_axes.shift(UP*1.3)\n",
    "        \n",
    "        # Axis labels\n",
    "        x_label = Text(\"Training Steps\", font_size=28)\n",
    "        x_label.next_to(loss_axes.x_axis, DOWN, buff=0.3)\n",
    "        y_label = Text(\"Loss\", font_size=28)\n",
    "        y_label.next_to(loss_axes.y_axis, LEFT, buff=0.3)\n",
    "        y_label.align_to(loss_axes.y_axis, UP)\n",
    "        y_label.shift(UP*0.5)\n",
    "        self.add(loss_axes, x_label, y_label)\n",
    "        \n",
    "        def get_loss_curve():\n",
    "            if len(loss_history_list) < 2:\n",
    "                return VGroup()\n",
    "            x_vals = np.arange(len(loss_history_list))\n",
    "            y_vals = np.array(loss_history_list)\n",
    "            return loss_axes.plot_line_graph(\n",
    "                x_values=x_vals,\n",
    "                y_values=y_vals,\n",
    "                add_vertex_dots=False,\n",
    "                line_color=YELLOW\n",
    "            )\n",
    "        \n",
    "        loss_curve = always_redraw(get_loss_curve)\n",
    "        self.add(loss_curve)\n",
    "\n",
    "        # 8. Gradient descent animation\n",
    "        for i in range(steps):\n",
    "            w1 = w_tracker.get_value()\n",
    "            w2 = v_tracker.get_value()\n",
    "            b0 = b_tracker.get_value()\n",
    "            \n",
    "            # Compute gradients\n",
    "            z = w1*X[:,0] + w2*X[:,1] + b0\n",
    "            y_pred = sigmoid(z)\n",
    "            grad_w1 = np.mean((y_pred - y) * X[:,0])\n",
    "            grad_w2 = np.mean((y_pred - y) * X[:,1])\n",
    "            grad_b = np.mean(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            new_w1 = w1 - lr * grad_w1\n",
    "            new_w2 = w2 - lr * grad_w2\n",
    "            new_b = b0 - lr * grad_b\n",
    "            \n",
    "            self.play(\n",
    "                w_tracker.animate.set_value(new_w1),\n",
    "                v_tracker.animate.set_value(new_w2),\n",
    "                b_tracker.animate.set_value(new_b),\n",
    "                run_time=0.035\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                self.wait(0.2)\n",
    "                \n",
    "            # Record loss history\n",
    "            loss_history_list.append(get_loss())\n",
    "        \n",
    "        self.wait(0.5)\n",
    "\n",
    "        # 9. Show final loss in yellow\n",
    "        self.remove(loss_text)\n",
    "        final_loss_text = Text(\n",
    "            f\"Final Loss: {get_loss():.3f}\", font_size=44, color=YELLOW, weight=BOLD\n",
    "        ).next_to(axes, DOWN, buff=0.5)\n",
    "        self.add(final_loss_text)\n",
    "        self.wait(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To render the animation in Jupyter, you would typically use:\n",
    "# %manim -v WARNING -qh LogisticRegressionDemo\n",
    "\n",
    "# Note: Requires Manim and Jupyter integration to be properly set up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
